{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will make a dictionary based sentiment analasys that looks at the data and decided whether or not the comment is positive or negative or somewhere in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"smallData.csv\")\n",
    "df['reviewText']= df['reviewText'].astype(str)\n",
    "ratings = list(df[\"overall\"])\n",
    "reviews = list(df[\"reviewText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are downloading the dictionary that has a comprehensive list of negative and positive words. We will use this dictionary to figure out whether a review is negative or positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\navneeth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('opinion_lexicon') #Download Opinion Dictionary\n",
    "positive_wds = set(opinion_lexicon.positive())\n",
    "negative_wds = set(opinion_lexicon.negative())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function sentence score that takes a sentence and looks through each word to see whether it is positive or negative. By looking at whether there are more positive or negative words, I am thinking that I can find the overall sentiment of the sentence (May be naive). For each sentence this below funciton would return a score of 1 or -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceScore(sentence):\n",
    "    filtered_words = []\n",
    "    lowercaseWords = []\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for word in sentence: #if alphanumeric, append word to list\n",
    "        if(word.isalnum()):\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    sentence = filtered_words\n",
    "    \n",
    "    sentence = filtered_words\n",
    "    \n",
    "    for word in sentence: #make all words lowercase\n",
    "        lowercaseWords.append(word.lower())\n",
    "    sentence = lowercaseWords\n",
    "\n",
    "    if not sentence:\n",
    "        return 0\n",
    "\n",
    "    total = len(sentence)\n",
    "    for word in sentence:\n",
    "        if(word in positive_wds):\n",
    "            pos += 1\n",
    "        if(word in negative_wds):\n",
    "            neg+=1\n",
    "    \n",
    "    return (pos-neg)/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below tests whether the above function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sen = \"Happy\"\n",
    "tokens = word_tokenize(sen)\n",
    "\n",
    "print(sentenceScore(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function that makes use of the above function and returns the average sentiment scores of a sentence in a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_review(review):\n",
    "    sentimentScores = [] #list with scores for all sentences in a review\n",
    "    sentences = sent_tokenize(review) #seperates a full review into multiple sentences\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence) #seperates a sentence into words\n",
    "        sentimentScores.append(sentenceScore(words))\n",
    "    return sum(sentimentScores)/len(sentimentScores) #returns average sentimentScores of a sentence in a review\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below puts everything together and runs the program. We write to a file named dictBasedSentiments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewSentiments = []\n",
    "for review in reviews:\n",
    "    reviewSentiments.append(score_review(review))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "        \"rating\": ratings,\n",
    "        \"review\": reviews,\n",
    "        \"review dictionary based sentiment\": reviewSentiments,\n",
    "})\n",
    "df.to_csv('dictBasedSentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to evaluate how well this program works in evaluating the sentiment of customer reviews by comparing it to the overall review. We will normalize both the prediction data and the customer review data and then compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_rating(value): #this normalizes the dictbasedsentiment data to a score of 0 for negative, 1 for neutral and 2 for positive\n",
    "    if value > 0.2:\n",
    "        return 2\n",
    "    if value <= 0.2 and value >= -0.2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df['predicted'] = df['review dictionary based sentiment'].apply(lambda x:score_to_rating(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_Target(value): #this normalizes the actual review rating data\n",
    "    if value >= 5:\n",
    "        return 2\n",
    "    if value <= 4 and value >= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df['target'] = df['rating'].apply(lambda x:score_to_Target(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to run all this and make a table evaluating prevision. We can see that there is a clear bias towards the neutral as the recall is so high. \n",
    "The low accuracy shows that this is not that precise of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.05      0.10      1500\n",
      "     neutral       0.36      0.93      0.52      1500\n",
      "    positive       0.78      0.28      0.41      1500\n",
      "\n",
      "    accuracy                           0.42      4500\n",
      "   macro avg       0.66      0.42      0.34      4500\n",
      "weighted avg       0.66      0.42      0.34      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rating_classes = list(df[\"target\"])\n",
    "sentiment_values  = list(df[\"predicted\"])\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "print(classification_report(rating_classes, sentiment_values, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
